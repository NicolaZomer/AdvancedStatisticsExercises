---
title: "Exercises Laboratory Session 04"
author: "Nicola Zomer"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
---

```{r setup-chunk}
knitr::opts_chunk$set(dev = "ragg_png")
options(digits=5) # set number of digits equal to 5

```

# Packages and functions

```{r, message=FALSE}

# tidyverse
library(tidyverse)

# others
library(kableExtra)
library(patchwork)
library(ggpubr)
library(latex2exp)
library(glue)
#library(gridExtra)

```


# Exercise 1 - Community Mobility Open Data
Community Mobility Reports have been created with the aim to provide insights into what has changed in response to policies aimed at combating COVID-19. Data can be found at https://www.google.com/covid19/mobility/.

Download and analyze the following data sets:

* https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv 
* https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip

Select a couple of European countries of your choice and analyze the trends in the variables over time: produce a plot of the data by averaging the observable over a period of one week (hint: convert the data field to `lubridate::week`) and one month and quantify the impact of COVID-19 restrictions on mobility situations.

```{r}

filepath='../../data/'

italy_20 <- read_csv(paste(filepath, '2020_IT_Region_Mobility_Report.csv', sep=''), col_names=TRUE)
italy_21 <- read_csv(paste(filepath, '2021_IT_Region_Mobility_Report.csv', sep=''), col_names=TRUE)
italy_22 <- read_csv(paste(filepath, '2022_IT_Region_Mobility_Report.csv', sep=''), col_names=TRUE)

uk_20 <- read_csv(paste(filepath, '2020_GB_Region_Mobility_Report.csv', sep=''), col_names=TRUE)
uk_21 <- read_csv(paste(filepath, '2021_GB_Region_Mobility_Report.csv', sep=''), col_names=TRUE)
uk_22 <- read_csv(paste(filepath, '2022_GB_Region_Mobility_Report.csv', sep=''), col_names=TRUE)

```


# Exercise 2 - Random number generators
One of the first random number generator was proposed by von Neumann, the so-called _middle square_ algorithm. 

Write R code to implement this type of generator and, given a fixed digit number input, square it an remove the leading and trailing digits, in order to return a number with the same number of digits as the original number.

```{r}

middle_square <- function(x.start=5772156649, n){
  input.digits <- length(unlist(strsplit(as.character(x.start),"")))
  
  # print(paste('Number of digits of the input number:', input.digits))
  if (x.start%%1 != 0) stop('The starting number must be integer')
  
  x.squared.char <- unlist(strsplit(as.character(x.start^2),""))  
  half.output.digits <- length(x.squared.char)%/%2
  
  if (input.digits%%2 ==0){ # even number of digits
    x.i <- x.squared.char[(half.output.digits-(input.digits/2-1)):(half.output.digits+input.digits/2)]
  } else{ # odd number of digits
    x.i <- x.squared.char[(half.output.digits-(input.digits%/%2)):(half.output.digits+input.digits%/%2)]
  }
  
  # check if the first digit is zero: in this case shift to the left
  i<-1
  while (x.i[1]=="0"){
    print(x.i)
    if (input.digits%%2 ==0){ # even number of digits
      x.i <- x.squared.char[(half.output.digits-(input.digits/2-1)-i):(half.output.digits+input.digits/2-i)]
    } else{ # odd number of digits
      x.i <- x.squared.char[(half.output.digits-(input.digits%/%2)-i):(half.output.digits+input.digits%/%2-i)]
    }
    i<-i+1
  }
  return(as.numeric(paste(x.i, collapse="")))
}

```

Generate 10 numbers using the _middle square_ algorithm:
```{r}

numbers <- numeric(11)
numbers[1] <- 5772156649

for (i in 1:10){
  numbers[i+1] <- middle_square(numbers[i])
}

data.frame(Iteration=0:10, Generated.Number=numbers) %>%
  kable() %>%
  kable_styling(full_width=FALSE)

```

# Exercise 3 - Bayesian Inference

A publishing company has recently launched a new journal. In order to determine how effective it is in reaching its possible audience, a market survey company selects a random sample of people from a possible target audience and interviews them. Out of 150 interviewed people, 29 have read the last issue of the journal.

### a) What kind of distribution would you assume for y, the number of people that have seen the last issue of the journal?

```{r}


```

### b) Assuming a uniform prior, what is the posterior distribution for y ?

```{r}


```

### c) Plot both posterior and likelihood ditributions functions

```{r}


```


# Exercise 4 - Bayesian Inference
A coin is flipped n = 30 times with the following outcomes:

T, T, T, T, T, H, T, T, H, H, T, T, H, H, H, T, H, T, H, T, H, H, T, H, T, H, T, H, H, H

Notice that there are 15 T and 15 H

### a) Assuming a flat prior, and a beta prior, plot the likelihood, prior and posterior distributions for the data set.

```{r}

n <- 30
r <- 15

nsamples <- 200
p <- seq(0, 1, length = nsamples)
delta.p <- 1/nsamples

unif.post <- dbinom(x=r, size=n, prob=p)
unif.post.norm <- unif.post/(delta.p*sum(unif.post))

ggplot()+
  geom_line(aes(x=p, y=unif.post.norm, color='Posterior and likelihood'), size=0.6)+
  geom_line(aes(x=p, y=dunif(p), color='Prior'), size=0.6)+
  labs(
    title='Flat prior', 
    x='p', 
    y=TeX('$P(p|r,n, M)$')
  )+
  scale_color_manual(name = "", values = c("Posterior and likelihood" = "steelblue", "Prior" = "firebrick"))+
  theme(legend.title= element_blank())+
  theme_bw()

```
```{r, fig.height=18, fig.width=18}

alpha <- 10; beta <- 10


gg_beta <- function(alpha, beta){
  ggplot()+
    geom_line(aes(x=p, y=dbeta(x=p, alpha+r, beta+n-r), color='Posterior'), size=1)+
    geom_line(aes(x=p, y=dbinom(x=r, size=n, prob=p), color='Likelihood'), size=1)+
    geom_line(aes(x=p, y=dbeta(p, alpha , beta), color='Prior'), size=1)+
    labs(
      title=paste('Beta prior (', alpha, ', ', beta, ')', sep=''), 
      x='p', 
      y=TeX('$P(p|r,n, M)$')
    )+
    scale_color_manual(name = "", values = c("Posterior" = "steelblue", "Likelihood"= "darkgreen", "Prior" = "firebrick"))+
    theme_bw()+
    theme(legend.title = element_blank(), 
          plot.title  = element_text(size=22), 
          axis.text   = element_text(size=18),
          axis.title  = element_text(size=18), 
          legend.text = element_text(size=22))
}

combined <- (gg_beta(1,1) +ylim(0,6) + gg_beta(1,5) +ylim(0,6) + gg_beta(1,10) +ylim(0,10) )/
            (gg_beta(5,1) +ylim(0,6) + gg_beta(5,5) +ylim(0,6) + gg_beta(5,10) +ylim(0,6))/ 
            (gg_beta(10,1)+ylim(0,10)+ gg_beta(10,5)+ylim(0,6) + gg_beta(10,10)+ylim(0,6)) & theme(legend.position = "bottom")

combined + plot_layout(guides = "collect")



```


### b) Evaluate the most probable value for the coin probability p and, integrating the posterior probability distribution, give an estimate for a 95% credibility interval.

```{r}

argmax <- function(x){
  max.x <- max(x)
  found <- FALSE
  
  imax <- 
  i <- 1
  while (!found & i<=length(x)){
    if (x[i]==max.x){
      found <- TRUE
      return(i)
    }
    i <- i+1
  }
}

# uniform
mpv.unif <- p[argmax(unif.post.norm)]
glue('Most probable value with an uniform prior: p={format(mpv.unif, digits=4)}')

# beta (10, 10)
beta.post.10 <- dbeta(x=p, 10+r, 10+n-r)
mpv.beta <- p[argmax(beta.post.10)]
glue('Most probable value with a beta prior (alpha=10, beta=10): p={format(mpv.beta ,digits=4)}')

```
```{r}

cred.interval <- function(p.post.funct, nsamples=200, perc=0.95){
    p <- seq(0, 1, length=nsamples)
    
    p.imax <- p[argmax(p.post.funct(p))]
    x.1.found <- FALSE
    x.2.found <- FALSE
    
    i<-1
    while (!x.1.found & p[i]<=p.imax){
      int.i <- integrate(p.post.funct, p[i], p.imax)$value
      int.iplus <- integrate(p.post.funct, p[i+1], p.imax)$value
      
      if (between(perc/2, int.iplus, int.i)){
        x.1.found <- TRUE
        x.1 <- (p[i]+p[i+1])/2
      }
      i <- i+1 
    }
    i <- nsamples
    while (!x.2.found & p[i]>=p.imax){
      int.i <- integrate(p.post.funct, p.imax, p[i])$value
      int.iminus <- integrate(p.post.funct, p.imax, p[i-1])$value
      
      if (between(perc/2, int.iminus, int.i)){
        x.2.found <- TRUE
        x.2 <- (p[i]+p[i-1])/2
      }
      i <- i-1 
    }
    return(c(x.1,x.2))
    
}

credlim.unif <- cred.interval(function(x){dbeta(x, 1+r, 1+n-r)}) # beta distribution with alpha=1, beta=1
credlim.beta.10 <- cred.interval(function(x){dbeta(x, 10+r, 10+n-r)})

print(credlim.unif)
print(credlim.beta.10)


```
```{r}

range.95.beta <- seq(credlim.beta.10[1], credlim.beta.10[2], length=200)
range.95.unif <- seq(credlim.unif[1], credlim.unif[2], length=200)

gg_credlim <- function(alpha, beta, range95, limits, Title='Posterior'){
  ggplot()+
  geom_line(aes(x=p, y=dbeta(x=p, alpha+r, beta+n-r)), size=1, color='steelblue')+
  geom_area(aes(x=range95, y=dbeta(x=range95, alpha+r, alpha+n-r)), fill='lightblue', color='steelblue', size=1)+
  labs(
    title=Title, 
    x='p', 
    y=TeX('$P(p|r,n, M)$')
  )+
  ylim(0, 6)+
  geom_vline(xintercept=limits, linetype='dashed')+
  annotate('text', x =limits[1]-0.06, y = 5, label=paste('x1=', round(limits[1], 2), sep=''))+
  annotate('text', x =limits[2]+0.06, y = 5, label=paste('x2=', round(limits[2], 2), sep=''))+
  theme_bw()
  
}

gg_credlim(10, 10, range.95.beta, credlim.beta.10, 'Posterior with beta (10,10) prior')
gg_credlim(1, 1, range.95.unif, credlim.unif, 'Posterior with uniform prior')


```


### c) Repeat the same analysis assuming a sequential analysis of the data. Show how the most probable value and the credibility interval change as a function of the number of coin tosses (i.e. from 1 to 30).

```{r}


```

### d) Do you get a different result, by analyzing the data sequentially with respect to a one-step analysis (i.e. considering all the data as a whole)?

```{r}

```













